{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNxPIyhNLQz+Kbczjdz6/SM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peremartra/FinLLMOpt/blob/FinChat-XS-Instruct/FinChat-XS/Finchat_Gradio_Interface.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio"
      ],
      "metadata": {
        "id": "TxN_zFk77459",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb7dd0c3-a54e-414f-953f-a5c12914fb29"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.1/322.1 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-OJ45Jtk7NEG"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Detect device: use CUDA if available, otherwise check for MPS (Apple Silicon), else CPU.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available() else \"cpu\"\n",
        "\n",
        "# Cache for loaded models to avoid reloading on every request.\n",
        "model_cache = {}"
      ],
      "metadata": {
        "id": "yIb_CBmU7cJ7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FINCHAT_NAME = \"oopere/FinChat-XS\""
      ],
      "metadata": {
        "id": "VzVh2Uq30jGa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# System prompts optimized for each model\n",
        "default_system_prompt = \"You are FinChat, a helpful AI assistant with expertise in finance. For general questions, respond naturally and concisely. Only use your financial knowledge when questions specifically relate to markets, investments, or financial concepts.\"\n",
        "MODEL_SYSTEM_PROMPTS = {\n",
        "    FINCHAT_NAME: default_system_prompt,\n",
        "\n",
        "    \"HuggingFaceTB/SmolLM2-360M-Instruct\": default_system_prompt,\n",
        "\n",
        "    \"meta-llama/Llama-3.2-1B-Instruct\": default_system_prompt\n",
        "}\n",
        "\n",
        "# Generation parameters optimized for each model\n",
        "MODEL_PARAMS = {\n",
        "    FINCHAT_NAME: {\n",
        "        \"max_new_tokens\": 600,\n",
        "        \"temperature\": 0.1,\n",
        "        \"repetition_penalty\":1.2,\n",
        "        \"top_p\": 1,\n",
        "    },\n",
        "    \"HuggingFaceTB/SmolLM2-360M-Instruct\": {\n",
        "        \"max_new_tokens\": 600,\n",
        "        \"temperature\": 0.1,\n",
        "        \"top_p\": 1,\n",
        "    },\n",
        "    \"meta-llama/Llama-3.2-1B-Instruct\": {\n",
        "        \"max_new_tokens\": 600,\n",
        "        \"temperature\": 0.1,\n",
        "        \"top_p\": 0.9,\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "97SwfHIZ7i63"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model_name):\n",
        "    \"\"\"\n",
        "    Loads and caches the tokenizer and model from Hugging Face.\n",
        "    \"\"\"\n",
        "    if model_name not in model_cache:\n",
        "        try:\n",
        "            print(f\"Loading {model_name} on {device} ...\")\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            # Ensure we have a pad token (using eos_token if not available)\n",
        "            if tokenizer.pad_token is None:\n",
        "                tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_name,\n",
        "                torch_dtype=torch.float16 if torch.cuda.is_available() else \"auto\"\n",
        "            )\n",
        "            model.to(device)\n",
        "            model_cache[model_name] = (tokenizer, model)\n",
        "            return tokenizer, model\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model: {str(e)}\")\n",
        "            raise e\n",
        "    return model_cache[model_name]\n",
        "\n",
        "def load_model_action(model_choice):\n",
        "    \"\"\"\n",
        "    Action triggered by the 'Load Model' button.\n",
        "    Loads the selected model and returns a status message.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        load_model(model_choice)\n",
        "        return (f\"Model '{model_choice}' loaded successfully.\", [])\n",
        "    except Exception as e:\n",
        "        return (f\"Error loading model '{model_choice}': {str(e)}\", [])\n",
        "\n",
        "def clean_assistant_response(text):\n",
        "    \"\"\"\n",
        "    Cleans up the assistant response by removing common prefixes.\n",
        "    More sophisticated than just stripping 'assistant'.\n",
        "    \"\"\"\n",
        "    # List of prefixes to remove\n",
        "    prefixes = ['assistant:', 'assistant', '<assistant>:', '<assistant>', 'AI:', 'FinChat:']\n",
        "\n",
        "    # Try each prefix\n",
        "    for prefix in prefixes:\n",
        "        if text.lower().startswith(prefix.lower()):\n",
        "            text = text[len(prefix):].strip()\n",
        "\n",
        "    return text.strip()\n"
      ],
      "metadata": {
        "id": "w9KSjoIG7pk8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat(model_choice, message, history):\n",
        "    \"\"\"\n",
        "    Handles the chat interaction with the selected model.\n",
        "    \"\"\"\n",
        "    # Initialize history as list of (user, assistant) tuples if not provided\n",
        "    if history is None or len(history) == 0:\n",
        "        history = []\n",
        "\n",
        "    # Exit early if no message\n",
        "    if not message.strip():\n",
        "        return history, history\n",
        "\n",
        "    try:\n",
        "        tokenizer, model = load_model(model_choice)\n",
        "\n",
        "        #Get system prompt for the selected model (or use default)\n",
        "        system_prompt = MODEL_SYSTEM_PROMPTS.get(\n",
        "            model_choice,\n",
        "            default_system_prompt\n",
        "        )\n",
        "\n",
        "        # Get generation parameters for the selected model (or use default)\n",
        "        gen_params = MODEL_PARAMS.get(\n",
        "            model_choice,\n",
        "            {\"max_new_tokens\": 150, \"temperature\": 0.2, \"top_p\": 0.9}\n",
        "        )\n",
        "\n",
        "        # Build conversation\n",
        "        conversation = [] #[{\"role\": \"system\", \"content\": system_prompt}]\n",
        "\n",
        "        # Add history\n",
        "        for user_text, bot_text in history:\n",
        "            conversation.append({\"role\": \"user\", \"content\": user_text})\n",
        "            conversation.append({\"role\": \"assistant\", \"content\": bot_text})\n",
        "\n",
        "        # Add new message\n",
        "        conversation.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "        # Try to use apply_chat_template safely\n",
        "        try:\n",
        "            input_text = tokenizer.apply_chat_template(conversation, tokenize=False)\n",
        "        except Exception as e:\n",
        "            print(f\"Error applying chat template: {e}\")\n",
        "            # Fallback for models without proper chat templates\n",
        "            input_text = f\"{system_prompt}\\n\\nUser: {message}\\nAssistant:\"\n",
        "\n",
        "        # Generate response\n",
        "        inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "        # Ensure input isn't too long\n",
        "        if inputs.shape[1] > tokenizer.model_max_length:\n",
        "            inputs = inputs[:, -tokenizer.model_max_length:]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output_ids = model.generate(\n",
        "                inputs,\n",
        "                **gen_params,\n",
        "                do_sample=True,\n",
        "            )\n",
        "\n",
        "        # Decode only the newly generated tokens\n",
        "        output_text = tokenizer.decode(output_ids[0][inputs.shape[1]:], skip_special_tokens=True)\n",
        "\n",
        "        # Clean up the response\n",
        "        output_text = clean_assistant_response(output_text)\n",
        "\n",
        "        # Update history\n",
        "        history.append((message, output_text))\n",
        "        return history, history\n",
        "\n",
        "    except Exception as e:\n",
        "        # Return error message as bot response\n",
        "        error_message = f\"Error: {str(e)}\"\n",
        "        history.append((message, error_message))\n",
        "        return history, history\n",
        "\n",
        "# Function to clear chat history\n",
        "def clear_history():\n",
        "    return [], []\n",
        "\n",
        "# Build the Gradio interface using Blocks\n",
        "with gr.Blocks(css=\"footer {visibility: hidden}\") as demo:\n",
        "    gr.Markdown(\"# FinChat Personal Finance Assistant\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=3):\n",
        "            model_choice = gr.Dropdown(\n",
        "                choices=[\n",
        "                    FINCHAT_NAME,\n",
        "                    \"HuggingFaceTB/SmolLM2-360M-Instruct\",\n",
        "                    \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "                ],\n",
        "                #value=FINCHAT_NAME,\n",
        "                label=\"Select Model\"\n",
        "            )\n",
        "\n",
        "        with gr.Column(scale=1):\n",
        "            load_button = gr.Button(\"Load Model\")\n",
        "\n",
        "        with gr.Column(scale=3):\n",
        "            load_status = gr.Textbox(label=\"Model Status\", interactive=False)\n",
        "\n",
        "    chatbot = gr.Chatbot(label=\"Chat Conversation\", height=200)\n",
        "\n",
        "    with gr.Row():\n",
        "        message = gr.Textbox(\n",
        "            label=\"Your Message\",\n",
        "            placeholder=\"Ask me about personal finance, budgeting, investments, etc.\",\n",
        "            lines=2\n",
        "        )\n",
        "\n",
        "    with gr.Row():\n",
        "        clear_btn = gr.Button(\"Clear Conversation\")\n",
        "        send_button = gr.Button(\"Send\", variant=\"primary\")\n",
        "\n",
        "    # State to store conversation history\n",
        "    state = gr.State([])\n",
        "\n",
        "    # Link buttons to functions\n",
        "    load_button.click(\n",
        "        fn=load_model_action,\n",
        "        inputs=model_choice,\n",
        "        outputs=[load_status, state]\n",
        "    )\n",
        "\n",
        "    clear_btn.click(\n",
        "        fn=clear_history,\n",
        "        inputs=[],\n",
        "        outputs=[chatbot, state]\n",
        "    )\n",
        "\n",
        "    # Link both the Send button and pressing Enter in the textbox to send a message\n",
        "    send_button.click(\n",
        "        fn=chat,\n",
        "        inputs=[model_choice, message, state],\n",
        "        outputs=[chatbot, state]\n",
        "    )\n",
        "\n",
        "    message.submit(\n",
        "        fn=chat,\n",
        "        inputs=[model_choice, message, state],\n",
        "        outputs=[chatbot, state]\n",
        "    )\n",
        "\n",
        "    # Load the default model on startup\n",
        "    #demo.load(\n",
        "    #    fn=load_model_action,\n",
        "    #    inputs=model_choice,\n",
        "    #    outputs=load_status\n",
        "    #)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiTCbDHq7uiv",
        "outputId": "593010bd-aed7-473b-d74c-bb0a37643c84"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/components/chatbot.py:285: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Launch the interface\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "VULjnbAJ7wbV",
        "outputId": "2857a198-6b47-41d8-c259-fe02ef87c9fc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://b6672befeb76c0c751.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://b6672befeb76c0c751.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WNOKX9Lf8jdS"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}