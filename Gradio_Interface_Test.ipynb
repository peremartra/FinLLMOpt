{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNvwEWGPm7Nfm0K/RZRUSsi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peremartra/FinLLMOpt/blob/FinChat-XS-Instruct/Gradio_Interface_Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FinChat-XS\n",
        "\n",
        "Example of use for FinChat. This notebook uses a Gradio interface to chat with FinChat and"
      ],
      "metadata": {
        "id": "B7bCRJMINtqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio"
      ],
      "metadata": {
        "id": "xQeIwcIQI1ny"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VZcIL6r0DH3d"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Detect device: use CUDA if available, otherwise check for MPS (Apple Silicon), else CPU.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available() else \"cpu\"\n",
        "\n",
        "# Cache for loaded models to avoid reloading on every request.\n",
        "model_cache = {}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model_name):\n",
        "    \"\"\"\n",
        "    Loads and caches the tokenizer and model from Hugging Face.\n",
        "    \"\"\"\n",
        "    if model_name not in model_cache:\n",
        "        print(f\"Loading {model_name} on {device} ...\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "        model.to(device)\n",
        "        model_cache[model_name] = (tokenizer, model)\n",
        "    return model_cache[model_name]\n",
        "\n",
        "def load_model_action(model_choice):\n",
        "    \"\"\"\n",
        "    Action triggered by the 'Load Model' button.\n",
        "    Loads the selected model and returns a status message.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        load_model(model_choice)\n",
        "        return (f\"Model '{model_choice}' loaded successfully.\", [])\n",
        "    except Exception as e:\n",
        "        return (f\"Error loading model '{model_choice}': {str(e)}\", [])\n",
        "\n"
      ],
      "metadata": {
        "id": "257aPllxGl8y"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat(model_choice, message, history):\n",
        "    \"\"\"\n",
        "    Appends the new message to the conversation history, builds the chat input\n",
        "    using the tokenizer's apply_chat_template method, generates a response, and returns\n",
        "    the updated conversation.\n",
        "    \"\"\"\n",
        "    # Initialize history as list of (user, assistant) tuples if not provided\n",
        "    if history is None:\n",
        "        history = []\n",
        "\n",
        "    # Convert the history (stored as tuples) into a list of message dictionaries\n",
        "    conversation = []\n",
        "    conversation.append({\"role\": \"system\", \"content\": \"\"\"You are FinChat, a specialized finance AI assistant. Provide accurate,\n",
        "concise information on markets, investments, accounting, and personal finance.\n",
        "Always clarify when financial information may vary by jurisdiction.\"\"\"})\n",
        "    for user_text, bot_text in history:\n",
        "        conversation.append({\"role\": \"user\", \"content\": user_text})\n",
        "        conversation.append({\"role\": \"assistant\", \"content\": bot_text})\n",
        "    # Append the new user message\n",
        "    conversation.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    tokenizer, model = load_model(model_choice)\n",
        "\n",
        "    # Format the conversation using the chat template method provided by the tokenizer.\n",
        "    # This method converts the list of message dicts into a model-specific formatted string.\n",
        "    input_text = tokenizer.apply_chat_template(conversation, tokenize=False)\n",
        "\n",
        "    # Encode the formatted input and generate a response\n",
        "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
        "    output_ids = model.generate(\n",
        "        inputs,\n",
        "        max_new_tokens=150,  # adjust as needed\n",
        "        do_sample=True,\n",
        "        temperature=0.2,\n",
        "        top_p=0.9,\n",
        "    )\n",
        "\n",
        "    # Decode only the newly generated tokens\n",
        "    output_text = tokenizer.decode(output_ids[0][inputs.shape[1]:], skip_special_tokens=True).strip()\n",
        "    # Add code to remove leading \"assistant:\" or similar prefixes\n",
        "    output_text = output_text.lstrip(\"assistant\").strip()  # Simple removal\n",
        "\n",
        "    # Append the assistant's reply to history (as a tuple for display)\n",
        "    history.append((message, output_text))\n",
        "    return history, history"
      ],
      "metadata": {
        "id": "T-Yz68cjc-6I"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the Gradio interface using Blocks.\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Hugging Face Model Chatbot\")\n",
        "    # Row for model selection and load button.\n",
        "    with gr.Row():\n",
        "        model_choice = gr.Dropdown(\n",
        "            choices=[\"oopere/FinChat5-XS\", \"HuggingFaceTB/SmolLM2-360M-Instruct\", \"meta-llama/Llama-3.2-1B-Instruct\"],\n",
        "            value=\"oopere/FinChat5-XS\",\n",
        "            label=\"Select Model\"\n",
        "        )\n",
        "        load_button = gr.Button(\"Load Model\")\n",
        "        load_status = gr.Textbox(label=\"Model Status\", interactive=False)\n",
        "\n",
        "    chatbot = gr.Chatbot(label=\"Chat Conversation\", height=200)\n",
        "\n",
        "    # Row for user input and send button.\n",
        "    with gr.Row():\n",
        "        message = gr.Textbox(label=\"Your Message\", placeholder=\"Type your message here...\", lines=1)\n",
        "        send_button = gr.Button(\"Send\")\n",
        "\n",
        "    # State to store conversation history.\n",
        "    state = gr.State([])\n",
        "\n",
        "    # Link the \"Load Model\" button to load the model and update the status.\n",
        "    load_button.click(fn=load_model_action, inputs=model_choice, outputs=[load_status, state])\n",
        "\n",
        "    # Link both the Send button and pressing Enter in the textbox to send a message.\n",
        "    send_button.click(fn=chat, inputs=[model_choice, message, state], outputs=[chatbot, state])\n",
        "    message.submit(fn=chat, inputs=[model_choice, message, state], outputs=[chatbot, state])"
      ],
      "metadata": {
        "id": "zZv6aOGUbyVH",
        "outputId": "fd65de9d-c376-4241-a16b-e38831bcd20e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/components/chatbot.py:285: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Launch the interface.\n",
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "gI7Wpl8-Ghgt",
        "outputId": "780ddf40-9e22-47d3-ce11-d3bc7a2d5385"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://8942a238c35721da58.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://8942a238c35721da58.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NvyIs6U8HIbr"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}